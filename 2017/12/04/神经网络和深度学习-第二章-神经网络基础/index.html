<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>神经网络和深度学习-第二章 神经网络基础 | Quincey | mind in mind</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="神经网络和深度学习,公开课">
    <meta name="description" content="Chapter2：神经网络基础二分分类 如Cat vs No-Cat问题，目标是训练一个分类器，输入是一张图片，图片被表示成一个特征向量x，并且预测标签y是1（Cat)还是0（No-Cat）。  吴恩达的矩阵表示法：用列向量表示一个样本，因此 X.shape==(n_x,m)，n_x表示特征数，m是样本大小。">
<meta name="keywords" content="神经网络和深度学习,公开课">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络和深度学习-第二章 神经网络基础">
<meta property="og:url" content="http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/index.html">
<meta property="og:site_name" content="Quincey">
<meta property="og:description" content="Chapter2：神经网络基础二分分类 如Cat vs No-Cat问题，目标是训练一个分类器，输入是一张图片，图片被表示成一个特征向量x，并且预测标签y是1（Cat)还是0（No-Cat）。  吴恩达的矩阵表示法：用列向量表示一个样本，因此 X.shape==(n_x,m)，n_x表示特征数，m是样本大小。">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4905573-cfe68b21b75cb648.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4905573-56de9aecf7ebe219.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4905573-a6c4423adc8d5eac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4905573-a95ff1975946e9ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://pic2.zhimg.com/50/v2-3f4959cd70308df496ecc4568a0d982d_hd.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Y">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L%28Y%2Cf%28X%29%29+%3D+%28Y-f%28X%29%29%5E2)，这个函数就称为损失函数(loss">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)肯定不是最好的，因为它过度学习历史数据，导致它在真正预测时效果会很不好，这种情况称为过拟合(over-fitting">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=min%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29%2B%5Clambda+J%28f%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=f_%7B2%7D%28x%29">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/4905573-936cfd756c0bf4bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://img.blog.csdn.net/20170926083150863?">
<meta property="og:image" content="http://img.blog.csdn.net/20170926083227668?">
<meta property="og:updated_time" content="2017-12-05T13:12:01.780Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络和深度学习-第二章 神经网络基础">
<meta name="twitter:description" content="Chapter2：神经网络基础二分分类 如Cat vs No-Cat问题，目标是训练一个分类器，输入是一张图片，图片被表示成一个特征向量x，并且预测标签y是1（Cat)还是0（No-Cat）。  吴恩达的矩阵表示法：用列向量表示一个样本，因此 X.shape==(n_x,m)，n_x表示特征数，m是样本大小。">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/4905573-cfe68b21b75cb648.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
    
        <link rel="alternate" type="application/atom+xml" title="Quincey" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="//unpkg.com/hexo-theme-material-indigo@latest/css/style.css">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/logo.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Quincey Svng</h5>
          <a href="mailto:salsa2010@foxmail.com" title="salsa2010@foxmail.com" class="mail">salsa2010@foxmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Quincey Svng
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/SuperFireFoxy" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="http://www.weibo.com/salsa2010" target="_blank" >
                <i class="icon icon-lg icon-weibo"></i>
                Weibo
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://www.facebook.com/felix.song.79" target="_blank" >
                <i class="icon icon-lg icon-link"></i>
                facebook
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">神经网络和深度学习-第二章 神经网络基础</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">神经网络和深度学习-第二章 神经网络基础</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-12-04T10:47:58.000Z" itemprop="datePublished" class="page-time">
  2017-12-04
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/神经网络和深度学习/">神经网络和深度学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter2：神经网络基础"><span class="post-toc-number">1.</span> <span class="post-toc-text">Chapter2：神经网络基础</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#二分分类"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">二分分类</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#logistic回归"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">logistic回归</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#logistic回归的损失函数"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">logistic回归的损失函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度下降法"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">梯度下降法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Derivatives"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">Derivatives</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#More-Derivative-Examples"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">More Derivative Examples</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Computation-graph"><span class="post-toc-number">1.7.</span> <span class="post-toc-text">Computation graph</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Derivatives-with-a-Computation-Graph"><span class="post-toc-number">1.8.</span> <span class="post-toc-text">Derivatives with a Computation Graph</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-神经网络和深度学习-第二章-神经网络基础"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">神经网络和深度学习-第二章 神经网络基础</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-12-04 18:47:58" datetime="2017-12-04T10:47:58.000Z"  itemprop="datePublished">2017-12-04</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/神经网络和深度学习/">神经网络和深度学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="Chapter2：神经网络基础"><a href="#Chapter2：神经网络基础" class="headerlink" title="Chapter2：神经网络基础"></a>Chapter2：神经网络基础</h1><h2 id="二分分类"><a href="#二分分类" class="headerlink" title="二分分类"></a>二分分类</h2><ul>
<li><p>如<code>Cat vs No-Cat</code>问题，目标是训练一个分类器，输入是一张图片，图片被表示成一个特征向量<code>x</code>，并且预测标签<code>y</code>是<code>1（Cat)</code>还是<code>0（No-Cat）</code>。</p>
</li>
<li><p>吴恩达的矩阵表示法：用列向量表示一个样本，因此 <code>X.shape==(n_x,m)</code>，<code>n_x</code>表示特征数，<code>m</code>是样本大小。</p>
</li>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/4905573-cfe68b21b75cb648.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ch2.1.cat](http://upload-images.jianshu.io/upload_images/4905573-6fd32189efefa94c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)![ch2.1.n_x" title="">
                </div>
                <div class="image-caption">ch2.1.cat](http://upload-images.jianshu.io/upload_images/4905573-6fd32189efefa94c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)![ch2.1.n_x</div>
            </figure>
</li>
</ul>
<h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a><code>logistic</code>回归</h2><ul>
<li><p><code>logistic</code>回归是一种监督学习下的学习算法，使得输出<code>y</code>要么都是<code>0</code>或者要么都是<code>1</code>。<code>logistic</code>回归的目标是使预测和训练数据之间的误差最小化。</p>
</li>
<li><p>对于<code>Cat vs No-Cat</code>问题，给定一张图的特征向量 <code>x</code>，这个算法将会评估这幅图是猫的概率：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/4905573-56de9aecf7ebe219.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ch2.2.P*(cat)](http://upload-images.jianshu.io/upload_images/4905573-6f1a2bd727f6a2ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)![ch2.2.Sigmoid function" title="">
                </div>
                <div class="image-caption">ch2.2.P*(cat)](http://upload-images.jianshu.io/upload_images/4905573-6f1a2bd727f6a2ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)![ch2.2.Sigmoid function</div>
            </figure></p>
</li>
<li><p>通过计算，<code>W^Tx+b</code>是一个线性函数<code>ax+b</code>（<code>W^T</code>计算出来是一个具体的数值<code>a</code>），因为我们期待一个在<code>[0,1]</code>区间的概率约束，所以<code>sigmoid</code>函数被使用。</p>
</li>
<li><p>sigmoid函数的性质：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/4905573-a6c4423adc8d5eac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ch2.2.sigmoid function property" title="">
                </div>
                <div class="image-caption">ch2.2.sigmoid function property</div>
            </figure></p>
</li>
</ul>
<h2 id="logistic回归的损失函数"><a href="#logistic回归的损失函数" class="headerlink" title="logistic回归的损失函数"></a><code>logistic</code>回归的损失函数</h2><ul>
<li><p>概念</p>
<ol>
<li>损失函数 Loss：单个样本的估计值与真实值的误差。</li>
<li>成本函数 Cost：所有样本的误差总和的平均值。</li>
</ol>
</li>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/4905573-a95ff1975946e9ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ch2.3.loss function & cost function" title="">
                </div>
                <div class="image-caption">ch2.3.loss function & cost function</div>
            </figure>
</li>
<li><p><strong>目标函数</strong>、<strong>损失函数</strong>、<strong>代价函数</strong>有什么区别</p>
<ul>
<li><p>首先给出结论：损失函数和代价函数是同一个东西，目标函数是一个与他们相关但更广的概念，对于目标函数来说在有约束条件下的最小化就是损失函数（loss function）。举个例子解释一下:（图片来自Andrew Ng Machine Learning公开课视频）<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://pic2.zhimg.com/50/v2-3f4959cd70308df496ecc4568a0d982d_hd.jpg" alt="function_pics" title="">
                </div>
                <div class="image-caption">function_pics</div>
            </figure></p>
</li>
<li><p>上面三个图的函数依次为<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=Y" alt="f1(x)](https://www.zhihu.com/equation?tex=f_%7B1%7D%28x%29),![f2(x)](https://www.zhihu.com/equation?tex=f_%7B2%7D%28x%29),![f3(x)](https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)。我们是想用这三个函数分别来拟合Price，Price的真实值记为![Y" title="">
                </div>
                <div class="image-caption">f1(x)](https://www.zhihu.com/equation?tex=f_%7B1%7D%28x%29),![f2(x)](https://www.zhihu.com/equation?tex=f_%7B2%7D%28x%29),![f3(x)](https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)。我们是想用这三个函数分别来拟合Price，Price的真实值记为![Y</div>
            </figure>。</p>
</li>
<li>我们给定<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=L%28Y%2Cf%28X%29%29+%3D+%28Y-f%28X%29%29%5E2)，这个函数就称为损失函数(loss" alt="x](https://www.zhihu.com/equation?tex=x)，这三个函数都会输出一个![f(X)](https://www.zhihu.com/equation?tex=f%28X%29),这个输出的![f(X)](https://www.zhihu.com/equation?tex=f%28X%29)与真实值![Y](https://www.zhihu.com/equation?tex=Y)可能是相同的，也可能是不同的，为了表示我们拟合的好坏，我们就用一个函数来度量拟合的程度，比如：![L(Y,f(X))" title="function)，或者叫代价函数(cost">
                </div>
                <div class="image-caption">function)，或者叫代价函数(cost</div>
            </figure>。损失函数越小，就代表模型拟合的越好。</li>
<li>那是不是我们的目标就只是让loss function越小越好呢？还不是。</li>
<li>这个时候还有一个概念叫风险函数(risk function)。风险函数是损失函数的期望，这是由于我们输入输出的<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29" alt="(X,Y)](https://www.zhihu.com/equation?tex=%28X%2CY%29)遵循一个联合分布，但是这个联合分布是未知的，所以无法计算。但是我们是有历史数据的，就是我们的训练集，![f(X)](https://www.zhihu.com/equation?tex=f%28X%29)关于训练集的平均损失称作经验风险(empirical risk)，即![empirical risk](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29)，所以我们的目标就是最小化![empirical risk" title="">
                </div>
                <div class="image-caption">(X,Y)](https://www.zhihu.com/equation?tex=%28X%2CY%29)遵循一个联合分布，但是这个联合分布是未知的，所以无法计算。但是我们是有历史数据的，就是我们的训练集，![f(X)](https://www.zhihu.com/equation?tex=f%28X%29)关于训练集的平均损失称作经验风险(empirical risk)，即![empirical risk](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29)，所以我们的目标就是最小化![empirical risk</div>
            </figure>，称为经验风险最小化。到这里完了吗？还没有。</li>
<li>如果到这一步就完了的话，那我们看上面的图，那肯定是最右面的<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)肯定不是最好的，因为它过度学习历史数据，导致它在真正预测时效果会很不好，这种情况称为过拟合(over-fitting" alt="f3(x)](https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)的经验风险函数最小了，因为它对历史的数据拟合的最好嘛。但是我们从图上来看![f3(x)" title="">
                </div>
                <div class="image-caption">f3(x)](https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)的经验风险函数最小了，因为它对历史的数据拟合的最好嘛。但是我们从图上来看![f3(x)</div>
            </figure>。</li>
<li>为什么会造成这种结果？大白话说就是它的函数太复杂了，都有四次方了，这就引出了下面的概念，我们不仅要让经验风险最小化，还要让<strong>结构风险最小化</strong>。这个时候就定义了一个函数<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=min%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7DL%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29%2B%5Clambda+J%28f%29" alt="J(f)](https://www.zhihu.com/equation?tex=J%28f%29)，这个函数专门用来**度量模型的复杂度**，在机器学习中也叫正则化(regularization)。常用的有![L1](https://www.zhihu.com/equation?tex=L_1),![L2](https://www.zhihu.com/equation?tex=L_2)范数。到这一步我们就可以说我们最终的优化函数是：![target function" title="">
                </div>
                <div class="image-caption">J(f)](https://www.zhihu.com/equation?tex=J%28f%29)，这个函数专门用来**度量模型的复杂度**，在机器学习中也叫正则化(regularization)。常用的有![L1](https://www.zhihu.com/equation?tex=L_1),![L2](https://www.zhihu.com/equation?tex=L_2)范数。到这一步我们就可以说我们最终的优化函数是：![target function</div>
            </figure>，即最优化经验风险和结构风险，而这个函数就被称为<strong>目标函数</strong>。</li>
<li>结合上面的例子来分析：最左面的<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://www.zhihu.com/equation?tex=f_%7B2%7D%28x%29" alt="f1(x)](https://www.zhihu.com/equation?tex=f_%7B1%7D%28x%29)结构风险最小（模型结构最简单），但是经验风险最大（对历史数据拟合的最差）；最右面的![f3(x)](https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)经验风险最小（对历史数据拟合的最好），但是结构风险最大（模型结构最复杂）;而![f2(x)" title="">
                </div>
                <div class="image-caption">f1(x)](https://www.zhihu.com/equation?tex=f_%7B1%7D%28x%29)结构风险最小（模型结构最简单），但是经验风险最大（对历史数据拟合的最差）；最右面的![f3(x)](https://www.zhihu.com/equation?tex=f_%7B3%7D%28x%29)经验风险最小（对历史数据拟合的最好），但是结构风险最大（模型结构最复杂）;而![f2(x)</div>
            </figure>达到了二者的良好平衡，最适合用来预测未知数据集。以上的理解基于Coursera上Andrew Ng的公开课和李航的《统计学习方法》,如有理解错误。</li>
</ul>
</li>
</ul>
<h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><ul>
<li><p>我们已经掌握了<code>Cost function</code>的表达式，接下来将使用梯度下降（<code>Gradient Descent</code>）算法来计算出合适的<code>w</code>和<code>b</code>值，从而最小化<code>m</code>个训练样本的<code>Cost function</code>，即<code>J(w,b)</code>。</p>
</li>
<li><p>由于<code>J(w,b)</code>是<code>convex function</code>，梯度下降算法是先随机选择一组参数<code>w</code>和<code>b</code>值，然后每次迭代的过程中分别沿着<code>w</code>和<code>b</code>的梯度（偏导数）的<strong>反方向</strong>前进一小步，不断修正<code>w</code>和<code>b</code>。每次迭代更新<code>w</code>和<code>b</code>后，都能让<code>J(w,b)</code>更接近全局最小值。梯度下降的过程如下图所示。</p>
</li>
<li><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="http://upload-images.jianshu.io/upload_images/4905573-936cfd756c0bf4bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ch2.4.Gradient Descent" title="">
                </div>
                <div class="image-caption">ch2.4.Gradient Descent</div>
            </figure>
</li>
<li><p>梯度下降算法每次迭代更新，w和b的修正表达式为：</p>
<p>  $$w:=w-\partial \frac{\partial J(w,b))}{\partial w}$$<br>  $$b:b-\partial \frac{\partial J(w,b))}{\partial b}$$</p>
<p>  上式中，$α$是学习因子（learning rate），表示梯度下降的步进长度。$α$大，$w$和$b$每次更新的“步伐”更大一些；$α$越小，$w$和$b$每次更新的“步伐”更小一些。在程序代码中，我们通常使用$dw$来表示$\frac{\partial J(w,b))}{\partial w}$，用$db$来表示$\frac{\partial J(w,b))}{\partial b}$。微积分里，$\frac{df}{dx}$表示对单一变量求导数，$\frac{\partial f}{\partial x}$表示对多个变量中某个变量求偏导数。</p>
<p>  梯度下降算法能够保证每次迭代w和b都能向着$J(w,b)$全局最小化的方向进行。</p>
</li>
</ul>
<h2 id="Derivatives"><a href="#Derivatives" class="headerlink" title="Derivatives"></a>Derivatives</h2><ul>
<li>这一部分的内容非常简单，Andrew主要是给对微积分、求导数不太清楚的同学介绍的。梯度或者导数一定程度上可以看成是斜率。关于求导数的方法这里就不再赘述了。</li>
</ul>
<h2 id="More-Derivative-Examples"><a href="#More-Derivative-Examples" class="headerlink" title="More Derivative Examples"></a>More Derivative Examples</h2><ul>
<li>Andrew给出了更加复杂的求导数的例子，略</li>
</ul>
<h2 id="Computation-graph"><a href="#Computation-graph" class="headerlink" title="Computation graph"></a>Computation graph</h2><ul>
<li><p>整个神经网络的训练过程实际上包含了两个过程：正向传播（Forward Propagation）和反向传播（Back Propagation）。正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，对参数$w$和$b$计算梯度的过程。下面，我们用计算图（Computation graph）的形式来理解这两个过程。</p>
</li>
<li><p>举个简单的例子，假如Cost function为$J(a,b,c)=3(a+bc)$，包含$a$，$b$，$c$三个变量。我们用$u$表示$bc$，$v$表示$a+u$，则$J=3v$。它的计算图可以写成如下图所示：</p>
<p>  <img src="http://img.blog.csdn.net/20170926083150863?" alt="ch2.7"></p>
<p>  令$a=5$，$b=3$，$c=2$，则$u=bc=6$，$v=a+u=11$，$J=3v=33$。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p>
</li>
</ul>
<h2 id="Derivatives-with-a-Computation-Graph"><a href="#Derivatives-with-a-Computation-Graph" class="headerlink" title="Derivatives with a Computation Graph"></a>Derivatives with a Computation Graph</h2><ul>
<li><p>上一部分介绍的是计算图的正向传播（Forward Propagation），下面我们来介绍其反向传播（Back Propagation），即计算输出对输入的偏导数。</p>
</li>
<li><p>还是上个计算图的例子，输入参数有3个，分别是a，b，c。</p>
<ul>
<li><p>首先计算J对参数a的偏导数。从计算图上来看，从右到左，J是v的函数，v是a的函数。则利用求导技巧，可以得到：</p>
<p>$$\frac{\partial J}{\partial a}=\frac{\partial J}{\partial v}\cdot \frac{\partial v}{\partial a}=3\cdot 1=3$$</p>
<p>根据这种思想，然后计算$J$对参数$b$的偏导数。从计算图上来看，从右到左，$J$是$v$的函数，$v$是$u$的函数，$u$是$b$的函数。可以推导：</p>
<p>$$\frac{\partial J}{\partial b}=\frac{\partial J}{\partial v}\cdot \frac{\partial v}{\partial u}\cdot \frac{\partial u}{\partial b}=3\cdot 1\cdot c=3\cdot 1\cdot 2=6$$</p>
<p>最后计算$J$对参数$c$的偏导数。仍从计算图上来看，从右到左，$J$是$v$的函数，$v$是$u$的函数，$u$是$c$的函数。可以推导：</p>
<p>$$\frac{\partial J}{\partial c}=\frac{\partial J}{\partial v}\cdot \frac{\partial v}{\partial u}\cdot \frac{\partial u}{\partial c}=3\cdot 1\cdot b=3\cdot 1\cdot 3=9$$</p>
<p>为了统一格式，在程序代码中，我们使用$da$，$db$，$dc$来表示$J$对参数$a$，$b$，$c$的偏导数。</p>
<p><img src="http://img.blog.csdn.net/20170926083227668?" alt="ch2.8"></p>
</li>
</ul>
</li>
</ul>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2017-12-05T13:12:01.780Z" itemprop="dateUpdated">2017-12-05 21:12:01</time>
</span><br>


        
        No one can guide you, except yourself!     self-link：<a href="/2017/12/04/神经网络和深度学习-第二章-神经网络基础/" target="_blank" rel="external">http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/</a>
        
    </div>
    <footer>
        <a href="http://matlabchina.cn">
            <img src="/img/logo.jpg" alt="Quincey Svng">
            Quincey Svng
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/公开课/">公开课</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络和深度学习/">神经网络和深度学习</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/&title=《神经网络和深度学习-第二章 神经网络基础》 — Quincey&pic=http://matlabchina.cn/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/&title=《神经网络和深度学习-第二章 神经网络基础》 — Quincey&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络和深度学习-第二章 神经网络基础》 — Quincey&url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/&via=http://matlabchina.cn" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/12/05/Thinking-in-Java-第七章-复用类/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">Thinking in Java 第七章 复用类</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/11/27/知乎-彼得·林奇（Peter-Lynch）的投资特点是什么？/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">知乎--彼得·林奇（Peter Lynch）的投资特点是什么？</h4>
      </a>
    </div>
  
</nav>



    








<section class="comments" id="comments">
    <div id="gitment_thread"></div>
    <link rel="stylesheet" href="//unpkg.com/gitment/style/default.css">
    <script src="//unpkg.com/gitment/dist/gitment.browser.js"></script>
    <script>
        var gitment = new Gitment({
            owner: 'SuperFireFoxy',
            repo: 'superfirefoxy.github.io',
            oauth: {
                client_id: 'f79ac80faa3d2c58bf61',
                client_secret: '655a9b1df478169a0c1f83cc4cc6b537873db4e3',
            },
        })
        gitment.render('comments')
    </script>
</section>







</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Quincey Svng &copy; 2017 - 2018</span>
            <span>
                
                <a href="http://www.miitbeian.gov.cn/" target="_blank">蜀ICP备16009264号-1</a><br>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/&title=《神经网络和深度学习-第二章 神经网络基础》 — Quincey&pic=http://matlabchina.cn/img/logo.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/&title=《神经网络和深度学习-第二章 神经网络基础》 — Quincey&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络和深度学习-第二章 神经网络基础》 — Quincey&url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/&via=http://matlabchina.cn" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://matlabchina.cn/2017/12/04/神经网络和深度学习-第二章-神经网络基础/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADM0lEQVR42u3aQXLjMAwEwPz/095rthLLM6CcsqjWKRVbEps+oIDh11d8Pb5dz/6T3JV8+vhxff9//vwTLmxsbOyLsB+HV77EZOk/v5OvJHlOcS82Njb2duzkcUmBWdmydpuO1/niXmxsbOwbs9uXJa1Csn35T4KNjY2N3T76faVu9gRsbGzse7KTodJKkJC3FivPecssDRsbG/vj2Tnm8/9+S76NjY2N/cHsR3mtNB7tGlYamxcKbGxs7I3YeQFIhvUrY/2V8GC4fmxsbOwt2CvHLvOBfl4Ck41OwokXRRQbGxv7Nux8rD+Lfo9LYAKbvQsbGxt7J3YbBuSlq13QShlrA2NsbGzsndjJDe2g57ikndu6tCX2v6ESNjY29g3YSZPQhgpnxQZtMFz8ntjY2NgXZM/i0rzkJLB82NQW3aetCDY2NvZ27JUQdzacmjUV7bDpRQHDxsbG3o6dH5Rph1Dt4pLoNx82/TJUwsbGxt6CnTcbK+FB3va0McCwFcHGxsbejt0enTluTtry1hae9YOb2NjY2Dux29HMyvHKtszkUUTdnGBjY2NvxF4pA3mpa9uPvECuxw/Y2NjYV2cnzcNsOpUvaL0JaUdX2NjY2PuxZ0Fv3hjknyZtTx4VvCXfxsbGxv4wdh4JnNWWtEtso4u8PGNjY2PvwW7bjOE5oHa+VcYGbQODjY2NvRO7HSG1g56VeLgGzDYXGxsb++LslZFNzss/bdubYcyMjY2NvR17FtnODkfO2oZZGXu6udjY2NibsvPjOyvHJfNwYhjfJtuHjY2NvRE7eXEyNjqr4OWDp3akVe83NjY29gXZyeNm4/vZuD8PlYfxMzY2NvYN2Ctj/fzTdqjUblBdwLCxsbEvy14JX2elscW0K4kKGDY2NvbF2Y/ymhWPNkJoY+M2kMDGxsbeiX1WpDore22QcG5cgY2Njb0Texa75sh8y/LfJF/P02diY2Njb8fOC8ysvTmrEM6GSr/chY2NjX17dlKo8mNA7aD/hHdhY2Nj35i9AvjLMvbi+9jY2NjbsdsWIj9wM4tsZwOj4hgQNjY29kbsvOTkr2kDhtlz2gKMjY2NvR37H6zuhpJ0sd7sAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/main.min.js"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="//unpkg.com/hexo-theme-material-indigo@latest/js/search.min.js" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = 'woops, lost connnection！';
            clearTimeout(titleTime);
        } else {
            document.title = 'bow, chaka bow boom!';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>



</body>
</html>
